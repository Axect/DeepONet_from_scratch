{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e927dd21-ed81-416a-bc44-44af7c15f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Scheduler - OneCycleLR, CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingLR\n",
    "\n",
    "# PyTorch Lightning\n",
    "import lightning as L\n",
    "\n",
    "# wandb\n",
    "import wandb\n",
    "\n",
    "# Ax - Hyperparameter Optimization\n",
    "from ax.service.ax_client import AxClient, ObjectiveProperties\n",
    "from ax.service.utils.report_utils import exp_to_df\n",
    "from ax.utils.notebook.plotting import init_notebook_plotting, render\n",
    "\n",
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14c5f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28d3086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5e31268-5fc0-4b17-b212-5f4006207887",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grf = pl.read_parquet(\"../data/grf_random_l.parquet\")\n",
    "df_grf_int = pl.read_parquet(\"../data/grf_random_l_int.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b658436a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = df_grf[\"group\"].n_unique()\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0da6d7da-2ba6-4624-85dd-05335b850f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10_000_000, 3)\n",
      "┌───────┬───────────┬───────┐\n",
      "│ x     ┆ grf       ┆ group │\n",
      "│ ---   ┆ ---       ┆ ---   │\n",
      "│ f64   ┆ f64       ┆ u64   │\n",
      "╞═══════╪═══════════╪═══════╡\n",
      "│ 0.0   ┆ -0.069628 ┆ 0     │\n",
      "│ 0.001 ┆ -0.067783 ┆ 0     │\n",
      "│ 0.002 ┆ -0.065942 ┆ 0     │\n",
      "│ 0.003 ┆ -0.064106 ┆ 0     │\n",
      "│ 0.004 ┆ -0.062274 ┆ 0     │\n",
      "│ …     ┆ …         ┆ …     │\n",
      "│ 0.996 ┆ 0.788142  ┆ 9999  │\n",
      "│ 0.997 ┆ 0.788338  ┆ 9999  │\n",
      "│ 0.998 ┆ 0.788525  ┆ 9999  │\n",
      "│ 0.999 ┆ 0.788706  ┆ 9999  │\n",
      "│ 1.0   ┆ 0.788879  ┆ 9999  │\n",
      "└───────┴───────────┴───────┘ shape: (1_000_000, 3)\n",
      "┌──────┬───────────┬───────┐\n",
      "│ y    ┆ grf_int   ┆ group │\n",
      "│ ---  ┆ ---       ┆ ---   │\n",
      "│ f64  ┆ f64       ┆ u64   │\n",
      "╞══════╪═══════════╪═══════╡\n",
      "│ 0.0  ┆ 0.0       ┆ 0     │\n",
      "│ 0.01 ┆ -0.000605 ┆ 0     │\n",
      "│ 0.02 ┆ -0.001029 ┆ 0     │\n",
      "│ 0.03 ┆ -0.001279 ┆ 0     │\n",
      "│ 0.04 ┆ -0.001359 ┆ 0     │\n",
      "│ …    ┆ …         ┆ …     │\n",
      "│ 0.96 ┆ 0.559707  ┆ 9999  │\n",
      "│ 0.97 ┆ 0.567493  ┆ 9999  │\n",
      "│ 0.98 ┆ 0.575317  ┆ 9999  │\n",
      "│ 0.99 ┆ 0.583172  ┆ 9999  │\n",
      "│ 1.0  ┆ 0.591051  ┆ 9999  │\n",
      "└──────┴───────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "print(df_grf, df_grf_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88448709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1_000_000, 3)\n",
      "┌──────┬───────────┬───────┐\n",
      "│ x    ┆ grf       ┆ group │\n",
      "│ ---  ┆ ---       ┆ ---   │\n",
      "│ f64  ┆ f64       ┆ u64   │\n",
      "╞══════╪═══════════╪═══════╡\n",
      "│ 0.0  ┆ -0.069628 ┆ 0     │\n",
      "│ 0.01 ┆ -0.051382 ┆ 0     │\n",
      "│ 0.02 ┆ -0.033627 ┆ 0     │\n",
      "│ 0.03 ┆ -0.016411 ┆ 0     │\n",
      "│ 0.04 ┆ 0.000218  ┆ 0     │\n",
      "│ …    ┆ …         ┆ …     │\n",
      "│ 0.96 ┆ 0.776535  ┆ 9999  │\n",
      "│ 0.97 ┆ 0.780622  ┆ 9999  │\n",
      "│ 0.98 ┆ 0.784061  ┆ 9999  │\n",
      "│ 0.99 ┆ 0.786822  ┆ 9999  │\n",
      "│ 1.0  ┆ 0.788879  ┆ 9999  │\n",
      "└──────┴───────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "df_grf = df_grf.filter(pl.col(\"x\").is_in([round(x * 0.01, 2) for x in range(101)]))\n",
    "print(df_grf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50950f6c-60cf-47e2-89ba-df8d606ef3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (100,), y: (10000, 100)\n",
      "grfs: (10000, 100), grf_ints: (10000, 100)\n"
     ]
    }
   ],
   "source": [
    "x = df_grf.filter(pl.col(\"group\") == 0)[\"x\"].to_numpy()\n",
    "y = df_grf_int.group_by(\"group\", maintain_order=True).agg(pl.col(\"y\"))[\"y\"].explode().to_numpy().reshape(n_samples, -1)\n",
    "grfs = df_grf.group_by(\"group\", maintain_order=True).agg(pl.col(\"grf\"))[\"grf\"].explode().to_numpy().reshape(n_samples, -1)\n",
    "grf_ints = df_grf_int.group_by(\"group\", maintain_order=True).agg(pl.col(\"grf_int\"))[\"grf_int\"].explode().to_numpy().reshape(n_samples, -1)\n",
    "\n",
    "y = y.astype(np.float32)\n",
    "grfs = grfs.astype(np.float32)\n",
    "grf_ints = grf_ints.astype(np.float32)\n",
    "\n",
    "print(f\"x: {x.shape}, y: {y.shape}\")\n",
    "print(f\"grfs: {grfs.shape}, grf_ints: {grf_ints.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22206689",
   "metadata": {},
   "source": [
    "## DeepONet from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d5525f",
   "metadata": {},
   "source": [
    "$$\n",
    "G: u \\in C[\\mathcal{D}] \\rightarrow G(u) \\in C[\\mathcal{R}] \\quad \\text{where } \\mathcal{D}, \\mathcal{R} \\text{ are compact}\n",
    "$$\n",
    "$$\n",
    "u(x) \\overset{G}{\\longrightarrow} G(u)(y) = \\int_0^y u(x) dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a671568",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(0.8 * n_samples)\n",
    "n_val = int(0.1 * n_samples)\n",
    "n_test = n_samples - n_train - n_val\n",
    "\n",
    "grf_train = grfs[:n_train]\n",
    "grf_val = grfs[n_train:n_train + n_val]\n",
    "grf_test = grfs[n_train + n_val:]\n",
    "\n",
    "y_train = y[:n_train]\n",
    "y_val = y[n_train:n_train + n_val]\n",
    "y_test = y[n_train + n_val:]\n",
    "\n",
    "grf_int_train = grf_ints[:n_train]\n",
    "grf_int_val = grf_ints[n_train:n_train + n_val]\n",
    "grf_int_test = grf_ints[n_train + n_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5b156b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegralData(Dataset):\n",
    "    def __init__(self, grf, y, grf_int):\n",
    "        self.grf = torch.tensor(grf)\n",
    "        self.y = torch.tensor(y)\n",
    "        self.grf_int = torch.tensor(grf_int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.grf)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.grf[idx], self.y[idx], self.grf_int[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5033bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = IntegralData(grf_train, y_train, grf_int_train)\n",
    "ds_val = IntegralData(grf_val, y_val, grf_int_val)\n",
    "ds_test = IntegralData(grf_test, y_test, grf_int_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f148ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepONetScratch(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        \n",
    "        num_input = hparams[\"num_input\"]\n",
    "        num_branch = hparams[\"num_branch\"]\n",
    "        num_output = hparams[\"num_output\"]\n",
    "        dim_output = hparams[\"dim_output\"]\n",
    "        hidden_size = hparams[\"hidden_size\"]\n",
    "        hidden_depth = hparams[\"hidden_depth\"]\n",
    "\n",
    "        branch_net = [nn.Linear(num_input, hidden_size), nn.GELU()]\n",
    "        for i in range(hidden_depth-1):\n",
    "            branch_net.append(nn.Linear(hidden_size, hidden_size))\n",
    "            branch_net.append(nn.GELU())\n",
    "        branch_net.append(nn.Linear(hidden_size, num_branch))\n",
    "        self.branch_net = nn.Sequential(*branch_net)\n",
    "\n",
    "        trunk_net = [nn.Linear(dim_output, hidden_size), nn.GELU()]\n",
    "        for _ in range(hidden_depth-1):\n",
    "            trunk_net.append(nn.Linear(hidden_size, hidden_size))\n",
    "            trunk_net.append(nn.GELU())\n",
    "        trunk_net.append(nn.Linear(hidden_size, num_branch))\n",
    "        self.trunk_net = nn.Sequential(*trunk_net)\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.randn(1), requires_grad=True)\n",
    "\n",
    "    def forward(self, u, y):\n",
    "        l = y.shape[1]\n",
    "        branch_out = self.branch_net(u)\n",
    "        trunk_out = torch.stack([self.trunk_net(y[:, i:i+1]) for i in range(l)], dim=2)\n",
    "        pred = torch.einsum(\"bp,bpl->bl\", branch_out, trunk_out) + self.bias\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba224ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, train_loader, val_loader, epochs, device):\n",
    "    model.to(device)\n",
    "\n",
    "    val_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for u, y, Guy in train_loader:\n",
    "            u, y, Guy = u.to(device), y.to(device), Guy.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(u, y)\n",
    "            loss = F.mse_loss(pred, Guy)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for u, y, Guy in val_loader:\n",
    "                u, y, Guy = u.to(device), y.to(device), Guy.to(device)\n",
    "                pred = model(u, y)\n",
    "                loss = F.mse_loss(pred, Guy)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss, \"epoch\": epoch+1})\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acdd49bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for u, y, Guy in test_loader:\n",
    "            u, y, Guy = u.to(device), y.to(device), Guy.to(device)\n",
    "            pred = model(u, y)\n",
    "            loss = F.mse_loss(pred, Guy)\n",
    "            test_loss += loss.item()\n",
    "    test_loss /= len(test_loader)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1146eb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train, batch_size=500, shuffle=True)\n",
    "dl_val = DataLoader(ds_val, batch_size=500)\n",
    "dl_test = DataLoader(ds_test, batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5561304",
   "metadata": {},
   "source": [
    "## Ax for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6a1ab3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "def evaluate_model(parameters):\n",
    "    hparams = {\n",
    "        \"num_input\": parameters.get(\"num_input\", 100),\n",
    "        \"num_branch\": parameters.get(\"num_branch\", 10),\n",
    "        \"num_output\": parameters.get(\"num_output\", 100),\n",
    "        \"dim_output\": parameters.get(\"dim_output\", 1),\n",
    "        \"hidden_size\": parameters.get(\"hidden_size\", 40),\n",
    "        \"hidden_depth\": parameters.get(\"hidden_depth\", 3),\n",
    "        \"learning_rate\": parameters.get(\"learning_rate\", 1e-2),\n",
    "        \"batch_size\": parameters.get(\"batch_size\", 500),\n",
    "        \"epochs\": parameters.get(\"epochs\", 200)\n",
    "    }\n",
    "    L.seed_everything(42)\n",
    "    model = DeepONetScratch(hparams)\n",
    "    \n",
    "    wandb.init(project=\"DeepONet-Ax\", config=hparams)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=hparams[\"learning_rate\"])\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=hparams[\"learning_rate\"], epochs=hparams[\"epochs\"], steps_per_epoch=len(dl_train) // hparams[\"batch_size\"] + 1)\n",
    "    val_loss = train(model, optimizer, scheduler, dl_train, dl_val, hparams[\"epochs\"], device)\n",
    "    test_loss = evaluate(model, dl_test, device)\n",
    "    \n",
    "    wandb.log({\"test_loss\": test_loss})\n",
    "    wandb.finish()\n",
    "\n",
    "    print(test_loss)\n",
    "    \n",
    "    return val_loss * 1e+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a70b14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client = AxClient(verbose_logging=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "465d54d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 03-24 15:57:08] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter learning_rate. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 03-24 15:57:08] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[FixedParameter(name='num_input', parameter_type=INT, value=100), ChoiceParameter(name='num_branch', parameter_type=INT, values=[10, 20, 30, 40], is_ordered=True, sort_values=False), FixedParameter(name='num_output', parameter_type=INT, value=100), FixedParameter(name='dim_output', parameter_type=INT, value=1), ChoiceParameter(name='hidden_size', parameter_type=INT, values=[40, 80, 120, 160], is_ordered=True, sort_values=False), ChoiceParameter(name='hidden_depth', parameter_type=INT, values=[2, 3, 4], is_ordered=True, sort_values=False), RangeParameter(name='learning_rate', parameter_type=FLOAT, range=[0.0001, 0.01], log_scale=True), FixedParameter(name='batch_size', parameter_type=INT, value=500), FixedParameter(name='epochs', parameter_type=INT, value=200)], parameter_constraints=[]).\n",
      "[INFO 03-24 15:57:08] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.\n",
      "[INFO 03-24 15:57:08] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=4 num_trials=None use_batch_trials=False\n",
      "[INFO 03-24 15:57:08] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=8\n",
      "[INFO 03-24 15:57:08] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=8\n",
      "[INFO 03-24 15:57:08] ax.modelbridge.dispatch_utils: `verbose`, `disable_progbar`, and `jit_compile` are not yet supported when using `choose_generation_strategy` with ModularBoTorchModel, dropping these arguments.\n",
      "[INFO 03-24 15:57:08] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 8 trials, BoTorch for subsequent trials]). Iterations after 8 will take longer to generate due to model-fitting.\n"
     ]
    }
   ],
   "source": [
    "ax_client.create_experiment(\n",
    "    name=\"DeepONet-Tuning\",\n",
    "    parameters=[\n",
    "        {\n",
    "            \"name\": 'num_input',\n",
    "            \"type\": 'fixed',\n",
    "            \"value\": 100,\n",
    "        },\n",
    "        {\n",
    "            \"name\": 'num_branch',\n",
    "            \"type\": 'choice',\n",
    "            \"values\": [10, 20, 30, 40],\n",
    "            \"value_type\": \"int\",\n",
    "            \"is_ordered\": True,\n",
    "            \"sort_values\": False,\n",
    "        },\n",
    "        {\n",
    "            \"name\": 'num_output',\n",
    "            \"type\": 'fixed',\n",
    "            \"value\": 100,\n",
    "        },\n",
    "        {\n",
    "            \"name\": 'dim_output',\n",
    "            \"type\": 'fixed',\n",
    "            \"value\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"name\": 'hidden_size',\n",
    "            \"type\": 'choice',\n",
    "            \"values\": [40, 80, 120, 160],\n",
    "            \"value_type\": \"int\",\n",
    "            \"is_ordered\": True,\n",
    "            \"sort_values\": False,\n",
    "        },\n",
    "        {\n",
    "            \"name\": 'hidden_depth',\n",
    "            \"type\": 'choice',\n",
    "            \"values\": [2, 3, 4],\n",
    "            \"value_type\": \"int\",\n",
    "            \"is_ordered\": True,\n",
    "            \"sort_values\": False,\n",
    "        },\n",
    "        {\n",
    "            \"name\": 'learning_rate',\n",
    "            \"type\": 'range',\n",
    "            \"bounds\": [1e-4, 1e-2],\n",
    "            \"log_scale\": True,\n",
    "        },\n",
    "        {\n",
    "            \"name\": 'batch_size',\n",
    "            \"type\": 'fixed',\n",
    "            \"value\": 500,\n",
    "        },\n",
    "        {\n",
    "            \"name\": 'epochs',\n",
    "            \"type\": 'fixed',\n",
    "            \"value\": 200,\n",
    "        },\n",
    "    ],\n",
    "    objectives={\"evaluate_model\": ObjectiveProperties(minimize=True)},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f726005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m      2\u001b[0m     parameters, trial_index \u001b[38;5;241m=\u001b[39m ax_client\u001b[38;5;241m.\u001b[39mget_next_trial()\n\u001b[0;32m----> 3\u001b[0m     ax_client\u001b[38;5;241m.\u001b[39mcomplete_trial(trial_index\u001b[38;5;241m=\u001b[39mtrial_index, raw_data\u001b[38;5;241m=\u001b[39m\u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[16], line 23\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(parameters)\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     22\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m OneCycleLR(optimizer, max_lr\u001b[38;5;241m=\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m], epochs\u001b[38;5;241m=\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m], steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dl_train) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m hparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m evaluate(model, dl_test, device)\n\u001b[1;32m     26\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_loss})\n",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, train_loader, val_loader, epochs, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(u, y)\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(pred, Guy)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Documents/Project/Machine_Learning/DeepONet_tutorial/.venv/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Project/Machine_Learning/DeepONet_tutorial/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    parameters, trial_index = ax_client.get_next_trial()\n",
    "    ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate_model(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ca3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters, values = ax_client.get_best_parameters()\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5065c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client.generation_strategy.trials_as_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79bca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "render(ax_client.get_optimization_trace())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa4e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client.get_best_trial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd1c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ax.plot.scatter import interact_fitted, plot_objective_vs_constraints, tile_fitted\n",
    "from ax.plot.slice import plot_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c8949",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_model = ax_client.generation_strategy.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422f17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "render(plot_slice(ax_model, \"learning_rate\", \"evaluate_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081567f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param, _ = ax_client.get_best_parameters()\n",
    "best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e4dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "render(ax_client.get_feature_importances())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dde8f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_df = ax_client.get_trials_data_frame()\n",
    "ax_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6868fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort dataframe by evaluate_model\n",
    "ax_df_sorted = ax_df.sort_values(\"evaluate_model\")\n",
    "ax_df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97012226",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client.get_best_trial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c0114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_df[(ax_df[\"learning_rate\"] > 0.0055) & (ax_df[\"learning_rate\"] < 0.0056)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3119eb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_df.iloc[20][\"learning_rate\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
